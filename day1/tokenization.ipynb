{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk spacy textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') #tokenization \n",
    "#nltk.download('stopwords') #stopwards removals\n",
    "#nltk.download('averaged_percenptron_tagger')#POS tagging \n",
    "#nltk.download('wordnet')#wordnet database and lemmatization\n",
    "#nltk.download('omw-1.4')  #stemming\n",
    "#nltk.download('indian') #Indian language POS tagging\n",
    "#nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectively'\n",
    "# Find average of ages mentioned in the above sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " '27',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = sent.split(sep=\" \")\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "27\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for  i in lst:\n",
    "    if i.isnumeric():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages= []\n",
    "for word in sent.split():\n",
    "    if word.isdigit():\n",
    "        ages.append(int(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ages)/len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = [int(word) for word in sent.split() if word.isdigit()]\n",
    "np.mean(ages)\n",
    "#sum(ages) / len(ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(word) for word in sent.split() if word.isnumeric()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent =\"Hello friends! How are you? Welcome to Python Programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the functions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segmentation \n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent) # segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of punctuation symbols wrt to words\n",
    "\n",
    "lst=word_tokenize(sent)\n",
    "punct_count =len([word for word in lst if not word.isalnum()])\n",
    "punct_count/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char ='tulsi Mundada'\n",
    "sys.getsizeof(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('T', 'U', 'L', 'S', 'I')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(84),chr(85),chr(76),chr(83),chr(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('M', 'A', 'N', 'A', 'S')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(77),chr(65),chr(78),chr(65),chr(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D', 'I', 'V', 'Y', 'A', 'N', 'I')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(68),chr(73),chr(86),chr(89),chr(65),chr(78),chr(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¤à¥à¤²à¤¸à¥€'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char='\\u0924\\u0941\\u0932\\u0938\\u0940'\n",
    "char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤®à¤¾à¤¨à¤¸'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char='\\u092E\\u093E\\u0928\\u0938'\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¦à¤¿à¤¯à¤¾'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char='\\u0926\\u093F\\u092F\\u093E'\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¤¿',\n",
       " 'à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤°',\n",
       " 'à¤®à¤¾à¤¨à¤¸',\n",
       " 'à¤…à¤—à¥à¤°à¤µà¤¾à¤²',\n",
       " 'à¤¤à¥à¤²à¤¸à¥€',\n",
       " 'à¤®à¥‚à¤‚à¤¦à¥œà¤¾',\n",
       " 'à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤°',\n",
       " 'à¤…à¤—à¥à¤°à¤µà¤¾à¤²']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name =\"à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¤¿ à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤° à¤®à¤¾à¤¨à¤¸ à¤…à¤—à¥à¤°à¤µà¤¾à¤² à¤¤à¥à¤²à¤¸à¥€ à¤®à¥‚à¤‚à¤¦à¥œà¤¾ à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤…à¤—à¥à¤°à¤µà¤¾à¤²\"\n",
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('à¤¦')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¥€ à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤° à¤®à¤¾à¤¨à¤¸ à¤…à¤—à¥à¤°à¤µà¤¾à¤² à¤¤à¥à¤²à¤¸à¥€ à¤®à¥‚à¤‚à¤¦à¥œà¤¾ à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤…à¤—à¥à¤°à¤µà¤¾à¤²'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¤¿ ','à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¥€ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤…à¤—à¥à¤°à¤µà¤¾à¤²\n",
      "à¤…à¤—à¥à¤°à¤µà¤¾à¤²\n"
     ]
    }
   ],
   "source": [
    "name =['à¤¦à¤¿à¤µà¥à¤¯à¤¾à¤¨à¤¿','à¤¸à¤‚à¤¤à¥‹à¤·à¤µà¤¾à¤°','à¤®à¤¾à¤¨à¤¸','à¤…à¤—à¥à¤°à¤µà¤¾à¤²','à¤¤à¥à¤²à¤¸à¥€','à¤®à¥‚à¤‚à¤¦à¥œà¤¾','à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤°','à¤…à¤—à¥à¤°à¤µà¤¾à¤²']\n",
    "for name in name:\n",
    "    if name.startswith('à¤…'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"à¤°à¤¾à¤œà¤—à¤¢à¤¼ à¤à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤•à¤¿à¤²à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯ à¤•à¥‡ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤­à¥€ à¤œà¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤²à¤—à¤­à¤— 26 à¤µà¤°à¥à¤·à¥‹à¤‚ à¤¤à¤• à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤•à¥‡ à¤¶à¤¾à¤¸à¤¨ à¤®à¥‡à¤‚ à¤®à¤°à¤¾à¤ à¤¾ à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¥à¥€à¥¤ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥‹ à¤°à¤¾à¤¯à¤—à¤¢à¤¼ Archived 2020-09-20 at the à¤µà¥‡à¤¬à¥ˆà¤• à¤®à¤¶à¥€à¤¨ à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¾à¤¨à¤¾à¤‚à¤¤à¤°à¤¿à¤¤ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤¤à¥‹à¤°à¤£ à¤¨à¤¾à¤®à¤• à¤à¤• à¤¨à¤¿à¤•à¤Ÿà¤µà¤°à¥à¤¤à¥€ à¤•à¤¿à¤²à¥‡ à¤¸à¥‡ à¤–à¥‹à¤œà¥‡ à¤—à¤ à¤–à¤œà¤¾à¤¨à¥‡ à¤•à¤¾ à¤‡à¤¸à¥à¤¤à¥‡à¤®à¤¾à¤² à¤‡à¤¸ à¤•à¤¿à¤²à¥‡ à¤•à¥‹ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤”à¤° à¤®à¤œà¤¬à¥‚à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤°à¤¾à¤œà¤—à¤¢à¤¼ à¤à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤•à¤¿à¤²à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯ à¤•à¥‡ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤­à¥€ à¤œà¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤²à¤—à¤­à¤— 26 à¤µà¤°à¥à¤·à¥‹à¤‚ à¤¤à¤• à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤•à¥‡ à¤¶à¤¾à¤¸à¤¨ à¤®à¥‡à¤‚ à¤®à¤°à¤¾à¤ à¤¾ à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¥à¥€à¥¤ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥‹ à¤°à¤¾à¤¯à¤—à¤¢à¤¼ Archived 2020-09-20 at the à¤µà¥‡à¤¬à¥ˆà¤• à¤®à¤¶à¥€à¤¨ à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¾à¤¨à¤¾à¤‚à¤¤à¤°à¤¿à¤¤ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤¤à¥‹à¤°à¤£ à¤¨à¤¾à¤®à¤• à¤à¤• à¤¨à¤¿à¤•à¤Ÿà¤µà¤°à¥à¤¤à¥€ à¤•à¤¿à¤²à¥‡ à¤¸à¥‡ à¤–à¥‹à¤œà¥‡ à¤—à¤ à¤–à¤œà¤¾à¤¨à¥‡ à¤•à¤¾ à¤‡à¤¸à¥à¤¤à¥‡à¤®à¤¾à¤² à¤‡à¤¸ à¤•à¤¿à¤²à¥‡ à¤•à¥‹ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤”à¤° à¤®à¤œà¤¬à¥‚à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¤¾à¤œà¤—à¤¢à¤¼',\n",
       " 'à¤à¤•',\n",
       " 'à¤ªà¤¹à¤¾à¤¡à¤¼à¥€',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤¹à¥ˆ',\n",
       " 'à¤œà¥‹',\n",
       " 'à¤­à¤¾à¤°à¤¤',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤œà¤¿à¤²à¥‡',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤‡à¤¸à¥‡',\n",
       " 'à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¨à¤¾à¤®',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤­à¥€',\n",
       " 'à¤œà¤¾à¤¨à¤¾',\n",
       " 'à¤œà¤¾à¤¤à¤¾',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤¯à¤¹',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤²à¤—à¤­à¤—',\n",
       " '26',\n",
       " 'à¤µà¤°à¥à¤·à¥‹à¤‚',\n",
       " 'à¤¤à¤•',\n",
       " 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¶à¤¾à¤¸à¤¨',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤®à¤°à¤¾à¤ à¤¾',\n",
       " 'à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯',\n",
       " 'à¤•à¥€',\n",
       " 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€',\n",
       " 'à¤¥à¥€à¥¤',\n",
       " 'à¤¬à¤¾à¤¦',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€',\n",
       " 'à¤•à¥‹',\n",
       " 'à¤°à¤¾à¤¯à¤—à¤¢à¤¼',\n",
       " 'Archived',\n",
       " '2020-09-20',\n",
       " 'at',\n",
       " 'the',\n",
       " 'à¤µà¥‡à¤¬à¥ˆà¤•',\n",
       " 'à¤®à¤¶à¥€à¤¨',\n",
       " 'à¤•à¤¿à¤²à¥‡',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¸à¥à¤¥à¤¾à¤¨à¤¾à¤‚à¤¤à¤°à¤¿à¤¤',\n",
       " 'à¤•à¤°',\n",
       " 'à¤¦à¤¿à¤¯à¤¾',\n",
       " 'à¤—à¤¯à¤¾',\n",
       " 'à¤¥à¤¾à¥¤',\n",
       " 'à¤¤à¥‹à¤°à¤£',\n",
       " 'à¤¨à¤¾à¤®à¤•',\n",
       " 'à¤à¤•',\n",
       " 'à¤¨à¤¿à¤•à¤Ÿà¤µà¤°à¥à¤¤à¥€',\n",
       " 'à¤•à¤¿à¤²à¥‡',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤–à¥‹à¤œà¥‡',\n",
       " 'à¤—à¤',\n",
       " 'à¤–à¤œà¤¾à¤¨à¥‡',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤‡à¤¸à¥à¤¤à¥‡à¤®à¤¾à¤²',\n",
       " 'à¤‡à¤¸',\n",
       " 'à¤•à¤¿à¤²à¥‡',\n",
       " 'à¤•à¥‹',\n",
       " 'à¤¬à¤¨à¤¾à¤¨à¥‡',\n",
       " 'à¤”à¤°',\n",
       " 'à¤®à¤œà¤¬à¥‚à¤¤',\n",
       " 'à¤•à¤°à¤¨à¥‡',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤²à¤¿à¤',\n",
       " 'à¤•à¤¿à¤¯à¤¾',\n",
       " 'à¤—à¤¯à¤¾',\n",
       " 'à¤¥à¤¾à¥¤']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¤¾à¤œà¤—à¤¢à¤¼ à¤à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤•à¤¿à¤²à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤­à¤¾à¤°à¤¤ à¤•à¥‡ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯ à¤•à¥‡ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤®à¥à¤°à¥à¤®à¤¦à¥‡à¤µ à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤­à¥€ à¤œà¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤²à¤—à¤­à¤— 26 à¤µà¤°à¥à¤·à¥‹à¤‚ à¤¤à¤• à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤•à¥‡ à¤¶à¤¾à¤¸à¤¨ à¤®à¥‡à¤‚ à¤®à¤°à¤¾à¤ à¤¾ à¤¸à¤¾à¤®à¥à¤°à¤¾à¤œà¥à¤¯ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¥à¥€à¥¤ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥‹ à¤°à¤¾à¤¯à¤—à¤¢à¤¼ Archived 2020-09-20 at the à¤µà¥‡à¤¬à¥ˆà¤• à¤®à¤¶à¥€à¤¨ à¤•à¤¿à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤¥à¤¾à¤¨à¤¾à¤‚à¤¤à¤°à¤¿à¤¤ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤¤à¥‹à¤°à¤£ à¤¨à¤¾à¤®à¤• à¤à¤• à¤¨à¤¿à¤•à¤Ÿà¤µà¤°à¥à¤¤à¥€ à¤•à¤¿à¤²à¥‡ à¤¸à¥‡ à¤–à¥‹à¤œà¥‡ à¤—à¤ à¤–à¤œà¤¾à¤¨à¥‡ à¤•à¤¾ à¤‡à¤¸à¥à¤¤à¥‡à¤®à¤¾à¤² à¤‡à¤¸ à¤•à¤¿à¤²à¥‡ à¤•à¥‹ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤”à¤° à¤®à¤œà¤¬à¥‚à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Space Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friends!ðŸ˜‚\\t How ðŸ«²are you ?â¤ï¸\\nWelcome to the worldâ˜ ï¸ of python ProgrammingðŸ’».\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(r\"C:\\Users\\dai\\Desktop\\NLP\\mydata.txt\" , encoding=\"UTF-8\")\n",
    "#print(file.read())\n",
    "data=file.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!ðŸ˜‚\\t',\n",
       " 'How',\n",
       " 'ðŸ«²are',\n",
       " 'you',\n",
       " '?â¤ï¸\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldâ˜ ï¸',\n",
       " 'of',\n",
       " 'python',\n",
       " 'ProgrammingðŸ’».\\n']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "#create the obj\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "#tokenize the dtaa\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friends!ðŸ˜‚\\t How ðŸ«²are you ?â¤ï¸\\nWelcome to the worldâ˜ ï¸ of python ProgrammingðŸ’».\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Tab Tokenizer\n",
    "\n",
    "file = open(r\"C:\\Users\\dai\\Desktop\\NLP\\mydata.txt\", encoding=\"UTF-8\")\n",
    "data=file.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!ðŸ˜‚',\n",
       " ' How ðŸ«²are you ?â¤ï¸\\nWelcome to the worldâ˜ ï¸ of python ProgrammingðŸ’».\\n']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "#create the obj\n",
    "tk = TabTokenizer()\n",
    "\n",
    "#tokenize the dtaa\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!ðŸ˜‚',\n",
       " 'How',\n",
       " 'ðŸ«²are',\n",
       " 'you',\n",
       " '?â¤ï¸',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldâ˜ ï¸',\n",
       " 'of',\n",
       " 'python',\n",
       " 'ProgrammingðŸ’».']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import LineTokenizer,WhitespaceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "#create the obj\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "#tokenize the dtaa\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWe tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 =\"the van rossel python creatorâ˜ ï¸ , visiting pune this week. ðŸ˜ˆthe development community is very eager to meet van rosselðŸ˜¶â€ðŸŒ«ï¸\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the van rossel python creatorâ˜ ï¸ , visiting pune this week. ðŸ˜ˆthe development community is very eager to meet van rosselðŸ˜¶â€ðŸŒ«ï¸\n"
     ]
    }
   ],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'van',\n",
       " 'rossel',\n",
       " 'python',\n",
       " 'creatorâ˜ ï¸',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'ðŸ˜ˆthe',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'van',\n",
       " 'rosselðŸ˜¶\\u200dðŸŒ«ï¸']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'van_rossel',\n",
       " 'python',\n",
       " 'creatorâ˜ ï¸',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'ðŸ˜ˆthe',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'van',\n",
       " 'rosselðŸ˜¶\\u200dðŸŒ«ï¸']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "#create the obj\n",
    "tk = MWETokenizer()\n",
    "\n",
    "#tokenize the dtaa\n",
    "tk.add_mwe((\"van\",\"rossel\"))\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'pythhon',\n",
       " 'Programming',\n",
       " ':D']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='Hello friends :) ! How are you ? Welcome to the world of pythhon Programming :D'\n",
    "\n",
    "#import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#create the obj\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "#tokenize the dtaa\n",
    "tk.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'ðŸ˜‚',\n",
       " 'How',\n",
       " 'ðŸ«²',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'â¤',\n",
       " 'ï¸',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'â˜ ',\n",
       " 'ï¸',\n",
       " 'of',\n",
       " 'python',\n",
       " 'Programming',\n",
       " 'ðŸ’»',\n",
       " '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\",text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok ? \"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens :\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://mitu.co.in/dataset\n",
    "# Download : student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(r'C:\\Users\\dai\\Desktop\\NLP\\student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roll\\tname\\tclass\\tmarks\\tage\\n1\\tanil\\tTE\\t56.77\\t22\\n2\\tamit\\tTE\\t59.77\\t21\\n3\\taniket\\tBE\\t76.88\\t19\\n4\\tajinkya\\tTE\\t69.66\\t20\\n5\\tasha\\tTE\\t63.28\\t20\\n6\\tayesha\\tBE\\t49.55\\t20\\n7\\tamar\\tBE\\t65.34\\t19\\n8\\tamita\\tBE\\t68.33\\t23\\n9\\tamol\\tTE\\t56.75\\t20\\n10\\tanmol\\tBE\\t78.66\\t21\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1\\tanil\\tTE\\t56.77\\t22', '2\\tamit\\tTE\\t59.77\\t21', '3\\taniket\\tBE\\t76.88\\t19', '4\\tajinkya\\tTE\\t69.66\\t20', '5\\tasha\\tTE\\t63.28\\t20', '6\\tayesha\\tBE\\t49.55\\t20', '7\\tamar\\tBE\\t65.34\\t19', '8\\tamita\\tBE\\t68.33\\t23', '9\\tamol\\tTE\\t56.75\\t20', '10\\tanmol\\tBE\\t78.66\\t21', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1', 'anil', 'TE', '56.77', '22'],\n",
       " ['2', 'amit', 'TE', '59.77', '21'],\n",
       " ['3', 'aniket', 'BE', '76.88', '19'],\n",
       " ['4', 'ajinkya', 'TE', '69.66', '20'],\n",
       " ['5', 'asha', 'TE', '63.28', '20'],\n",
       " ['6', 'ayesha', 'BE', '49.55', '20'],\n",
       " ['7', 'amar', 'BE', '65.34', '19'],\n",
       " ['8', 'amita', 'BE', '68.33', '23'],\n",
       " ['9', 'amol', 'TE', '56.75', '20'],\n",
       " ['10', 'anmol', 'BE', '78.66', '21'],\n",
       " ['']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data.split(\"\\n\")\n",
    "print(x)\n",
    "lst=[]\n",
    "for i in x.split(\"\\t\"):\n",
    "    if i.isdigit():\n",
    "        lst.append(int(i))\n",
    "    lst.append(sub)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roll', 'name', 'class', 'marks', 'age'],\n",
       " [1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata =[]\n",
    "for x in data.split(\"\\n\"):\n",
    "    inner_list = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    newdata.append(inner_list)\n",
    "newdata\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
